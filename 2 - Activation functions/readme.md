EXPERIMENT-2

Title: Activation functions to train Neural Network

Aim: Develop a Python program to implement various activation functions, including the sigmoid, tanh (hyperbolic tangent), ReLU (Rectified Linear Unit), Leaky ReLU, and softmax. The program should include functions to compute the output of each activation function for a given input. Additionally, it should be capable of plotting graphs representing the output of each activation function over a range of input values.

Tools: None

Procedure:
1) Check data that is linearly separable or not.
2) Analyze the activation functions.
3) Set up code for plotting
